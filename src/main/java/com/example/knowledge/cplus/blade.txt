delete-only 指的是 schema 元素的存在性只对删除操作可见。

例如当某索引处于 delete-only 状态时，F1 服务器中执行对应表的删除一行数据操作时能“看到”该索引，所以会同时删除该行对应的索引，与之相对的，如果是插入一行数据则“看不到”该索引，所以 F1 不会尝试新增该行对应的索引。

具体的，如果 schema 元素是 table 或 column ，该 schema 元素只对 delete 语句生效；如果 schema 元素是 index ，则只对 delete 和 update 语句生效，其中 update 语句修改 index 的过程可以认为是先 delete 后再 insert ，在 delete-only 状态时只处理其中的 delete 而忽略掉 insert 。

总之，只要某 schema 元素处于 delete-only 状态，F1 保证该 schema 元素对应的 kv 对总是能够被正确地删除，并且不会为此 schema 元素创建任何新的 kv 对。

write-only 指的是 schema 元素对写操作可见，对读操作不可见。

例如当某索引处于 write-only 状态时，不论是 insert 、 delete ，或是 update ，F1 都保证正确的更新索引，只是对于查询来说该索引仍是不存在的。

简单的归纳下就是 write-only 状态的 schema 元素可写不可读。




TiDB Server
TiDB Server 负责接收 SQL 请求，处理 SQL 相关的逻辑，并通过 PD 找到存储计算所需数据的 TiKV 地址，
与 TiKV 交互获取数据，最终返回结果。TiDB Server 是无状态的，其本身并不存储数据，只负责计算，
可以无限水平扩展，可以通过负载均衡组件（如LVS、HAProxy 或 F5）对外提供统一的接入地址。

PD Server
Placement Driver (简称 PD) 是整个集群的管理模块， PD 是一个集群，通过 Raft 协议保持数据的一致性，
单个实例失效时，如果这个实例不是 Raft 的 leader，那么服务完全不受影响；
如果这个实例是 Raft 的 leader，会重新选出新的 Raft leader，自动恢复服务。
PD 在选举的过程中无法对外提供服务，这个时间大约是3秒钟。
推荐至少部署三个 PD 实例，单个实例失效后，重启这个实例或者添加新的实例。其主要工作有三个：
一是存储集群的元信息（某个 Key 存储在哪个 TiKV 节点）；
二是对 TiKV 集群进行调度和负载均衡（如数据的迁移、Raft group leader 的迁移等）；
三是分配全局唯一且递增的事务 ID。
PD 是一个集群，需要部署奇数个节点，一般线上推荐至少部署 3 个节点。

TiKV Server
TiKV Server 负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。
存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，
每个 TiKV 节点会负责多个 Region。TiKV 使用 Raft 协议做复制，保持数据的一致性和容灾。
副本以 Region 为单位进行管理，不同节点上的多个 Region 构成一个 Raft Group，互为副本。
数据在多个 TiKV 之间的负载均衡由 PD 调度，这里也是以 Region 为单位进行调度。

TiSpark
TiSpark 作为 TiDB 中解决用户复杂 OLAP 需求的主要组件，将 Spark SQL 直接运行在 TiDB 存储层上，
同时融合 TiKV 分布式集群的优势，并融入大数据社区生态。至此，TiDB 可以通过一套系统，
同时支持 OLTP 与 OLAP，免除用户数据同步的烦恼。


2PL:https://my.oschina.net/alchemystar/blog/1438839
    https://houbb.github.io/2018/09/01/sql-2pl

    Percolator:https://my.oschina.net/fileoptions/blog/888995
               https://www.jianshu.com/p/721df5b4454b
               https://my.oschina.net/zhaiyuan/blog/2239774


               首先，Percolator 需要一个服务 timestamp oracle (TSO) 来分配全局的 timestamp，这个 timestamp 是按照时间单调递增的，而且全局唯一。
               任何事务在开始的时候会先拿一个 start timestamp (startTS)，然后在事务提交的时候会拿一个 commit timestamp (commitTS)。

               Percolator 提供三个 column family (CF)，Lock，Data 和 Write，当写入一个 key-value 的时候，会将这个 key 的 lock 放到 Lock CF 里面，
               会将实际的 value 放到 Data CF 里面，如果这次写入 commit 成功，则会将对应的 commit 信息放到入 Write CF 里面。

               Key 在 Data CF 和 Write CF 里面存放的时候，会把对应的时间戳给加到 Key 的后面。在 Data CF 里面，添加的是 startTS，而在 Write CF 里面，则是 commitCF。

               假设我们需要写入 a = 1，首先从 TSO 上面拿到一个 startTS，譬如 10，然后我们进入 Percolator 的 PreWrite 阶段，在 Lock 和 Data CF 上面写入数据，如下：












               Columns in Bigtable
                     Percolator在BigTable上抽象了五个Columns，其中三个跟事务相关，其定义如下

               Lock
                    事务产生的锁，未提交的事务会写本项，会包含primary lock的位置。其映射关系为
                    ${key,start_ts}=>${primary_key,lock_type,..etc}
                    ${key} 数据的key
                    ${start_ts} 事务开始时间
                    ${primary} 该事务的primary的引用. primary是在事务执行时，从待修改的keys中选择一个作为primary,其余的则作为secondary.
               Write
                    已提交的数据信息，存储数据所对应的时间戳。其映射关系为
                    ${key,commit_ts}=>${start_ts}
                    ${key} 数据的key
                    ${commit_ts} 事务的提交时间
                    ${start_ts} 该事务的开始时间,指向该数据在data中的实际存储位置。
                Data
                    具体存储数据集，映射关系为
                    ${key,start_ts} => ${value}
                    ${key} 真实的key
                    ${start_ts} 对应事务的开始时间
                    ${value} 真实的数据值



                    Client 将请求发送到 Leader 后，Leader 将请求作为一个 Proposal 通过 Raft 复制到自身以及 Follower 的 Log 中，
                    然后将其 commit。TiKV 将 commit 的 Log 应用到 RocksDB 上，由于 Input（即 Log）都一样，可推出各个 TiKV 的状态机（即 RocksDB）的状态能达成一致。
                    但实际多个 TiKV 不能保证同时将某一个 Log 应用到 RocksDB 上，也就是说各个节点不能实时一致，加之 Leader 会在不同节点之间切换，
                    所以 Leader 的状态机也不总有最新的状态。Leader 处理请求时稍有不慎，没有在最新的状态上进行，这会导致整个系统违反线性一致性。
                    好在有一个很简单的解决方法：依次应用 Log，将应用后的结果返回给 Client。


                    Peer
                    Peer 封装了 Raft RawNode，我们对 Raft 的 Propose，ready 的处理都是在 Peer 里面完成的。

                    首先关注 propose 函数，Peer 的 propose 是外部 Client command 的入口。Peer 会判断这个 command 的类型：

                    如果是只读操作，并且 Leader 仍然是在 lease 有效期内，Leader 就能直接提供 local read，不需要走 Raft 流程。

                    如果是 Transfer Leader 操作，Peer 首先会判断自己还是不是 Leader，同时判断需要变成新 Leader 的 Follower 是不是有足够新的 Log，
                    如果条件都满足，Peer 就会调用 RawNode 的 transfer_leader 命令。

                    如果是 Change Peer 操作，Peer 就会调用 RawNode propose_conf_change。

                    剩下的，Peer 会直接调用 RawNode 的 propose。

